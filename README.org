* DAMP2012 Branch. 
  The state of this source as it was at the writing of the DAMP2012 article.

* GCDObsidian
  + Embedded DSL for implementation of GPU Kernels
  + Generates CUDA/OpenCL kernels
  + Generates Sequential C code for testing purposes
    
* News in GCDObsidian 
  + Push Arrays: Explanation to come

  + (BROKEN!!!) SyncAnalysis: Trying to automatically decide when to insert 
    a CUDA __syncthreads(); call. Previously everytime the programmer 
    did a sync it resulted in a CUDA __syncthreads() in the generated 
    CUDA code. 
  
  + Kernel Building blocks: 
    There are pure functions, (Array a -> Array a), that can be composed
    using Haskell functional composition. These are turned into "kernels" using the function pure.
    
    
* Work in progress
  + More library functions needed
  + Library of Pushy array functions needed too.
    Should provide a lot of room for experimentation.
    
* Latest changes
  + Trying to add the idea of Push arrays to Obsidian. 
  + What used to be (Array Int) is now (Array (Exp Int) or 
    (Array (Data Int)), whichever is preferable. 
    This means the Array datatype changed from 
    data Array a = Array (Exp Word32 -> Exp a) Word32 
    to 
    data Array a = Array (Exp Word32 -> a) Word32 
    
  + LLArrays have been removed along with many hundreds 
    of lines of code that needs to be completely rewritten. 
    The push array takes the role of the old LLArray in this version. 


* Notes
  + (Solved, it works, see below)
    OpenCL: The kind of pointer arithmetic + casting, that has worked so well 
    in CUDA, seems to not work in OpenCL on the local memory. 
    Will need to scan the specification/documentation to find exactly what is 
    allowed and not on pointers to local memory in an OpenCL kernel. (Solved, see below)

  + The address space qualifiers are important! (__local int*)shared vs (int*)shared. 
    (int*)shared tried to cast local memory array called shared into a global array, so illegal!    

  + Obsidian Int are "64"bit on 64bit architectures. Is it also true that 
    a CUDA int is 64bit on 64bit architectures?

* TODO:
  + Fix SyncAnalysis
  + Perform CSE
  + Kernel Coordination layer
  + Generate the vSwap/iSwap kind of kernels. That is Kernels that operate 
       on an array based on its global tid instead of local tid.
  +  Generate Kernels that take scalar parameters
  + Syncing needs to be improved. (work on more kinds of arrays, needs type class magic)
  + Push arrays and parallelism 
  + Filters and unsafe operations that lead to nondeterminism
